data_bin: data-bin/gigaword_ref

arch: length_control_at_multiply_emb

task: translation_rouge_lc

length_control: true

ddp_backend: c10d
criterion: label_smoothed_cross_entropy
label_smoothing: 0.1
optimizer: adam
lr: 3e-4
lr_scheduler: inverse_sqrt
warmup_init_lr: '1e-07'
stop_min_lr: '1e-09'

# 4 gpus
# max_tokens: 8192
# update_freq: 5

max_tokens: 8192
update_freq: 4

# left_pad_source: true
# left_pad_target: true

# adam_betas: '"(0.9, 0.98)"'

dropout: 0.3
weight_decay: 0.01

# model hyperparameter
activation_fn: gelu
share_all_embeddings: true

encoder_normalize_before: true
decoder_normalize_before: true

decoder_learned_pos: true
encoder_learned_pos: true

# fp16_scale_tolerance: 1.0


# Train config
seed: 0

log_format: simple
log_interval: 100

warmup_updates: 10000
max_update: 300000

fp16: true
clip_norm: 1.0


keep_best_checkpoints: 5

validate_interval: 300
save_interval: 300
validate_interval_updates: 300
save_interval_updates: 300

keep_interval_updates: 1
keep_last_epochs: 1

skip_invalid_size_inputs_valid_test: true

valid_subset: valid2K
ignore_unused_valid_subsets: true
best_checkpoint_metric: rouge-1
eval_rouge: true
eval_rouge_print_samples: true
eval_rouge_remove_bpe: true
eval_rouge_detok: moses
eval_tokenized_rouge: true
maximize_best_checkpoint_metric: true
eval_bleu_args: "'{\"beam\": 4, \"max_len_a\": 1.2, \"max_len_b\": 10}'"

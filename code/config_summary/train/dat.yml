# Path
data_bin: data-bin/gigaword_ref
# save_dir: checkpoints_tmp
tensorboard_logdir: false

# log_file: iwslt_liwei.out


# language config
source_lang: article
target_lang: summary


# model arch set up
arch: glat_decomposed_link_base
task: translation_ctc_rouge
ddp_backend: c10d
criterion: nat_dag_loss
noise: full_mask

# data filtering
src_upsample_for_task: 1

# optimization
optimizer: adam
adam_betas: '"(0.9, 0.98)"'
label_smoothing: 0

dropout: 0.3
weight_decay: 0.01

# model hyperparameter
activation_fn: gelu
share_all_embeddings: true

decoder_learned_pos: true
encoder_learned_pos: true
apply_bert_init: true


# Train config
seed: 0

log_format: simple
log_interval: 200

max_tokens: 8192
update_freq: 6

warmup_updates: 10000
max_update: 300000
glat_p: '0.5:0.2@300k'

fp16: true
clip_norm: 0.1
lr: 0.0005
warmup_init_lr: '1e-07'
stop_min_lr: '1e-09'
lr_scheduler: inverse_sqrt

grouped_shuffling: true

left_pad_source: False
left_pad_target: False

# valid config
fixed_validation_seed: 7

max_tokens_valid: 16384
valid_subset: valid

validate_interval: 500
save_interval: 500
validate_interval_updates: 500
save_interval_updates: 500
keep_interval_updates: 5
keep_last_epochs: 5
keep_best_checkpoints: 5

skip_invalid_size_inputs_valid_test: true

eval_rouge: true
eval_rouge_print_samples: true
eval_rouge_remove_bpe: true
eval_rouge_detok: moses
eval_tokenized_rouge: true
best_checkpoint_metric: rouge-l
maximize_best_checkpoint_metric: true


links_feature: 'feature:position'
decode_strategy: 'lookahead'
max_source_positions: 3096
max_target_positions: 512
src_upsample_scale: 1
glance_strategy: 'number-random'
max_transition_length: 99999
# min_sent_length: 5
length_loss_factor: 0


eval_bleu_args: "'{\"iter_decode_max_iter\": 0, \"iter_decode_with_beam\": 1}'"


# others
user_dir: fs_plugins




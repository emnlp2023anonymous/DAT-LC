# Path
data_bin: data-bin/gigaword_ref
# save_dir: checkpoints_tmp
tensorboard_logdir: false

# log_file: iwslt_liwei.out


# language config
source_lang: article
target_lang: summary


# model arch set up
arch: nat_ctc_encoder_only
task: translation_ctc_rouge
ddp_backend: legacy_ddp
criterion: nat_loss
noise: full_mask
remove_bos_eos_tgt: true

# data filtering
src_upsample_for_task: 1
src_upsample_scale: 1
plain_ctc: true

# optimization
optimizer: adam
adam_betas: '"(0.9, 0.98)"'
label_smoothing: 0

dropout: 0.1
weight_decay: 0.01

# model hyperparameter
activation_fn: gelu
share_all_embeddings: true

decoder_learned_pos: true
encoder_learned_pos: true
apply_bert_init: true

encoder_layers: 6
decoder_layers: 0

# Train config
seed: 0

log_format: simple
log_interval: 200

max_tokens: 8192
update_freq: 5

warmup_updates: 10000
max_update: 300000

fp16: true
clip_norm: 0.1
lr: 0.0005
warmup_init_lr: '1e-07'
stop_min_lr: '1e-09'
lr_scheduler: inverse_sqrt

grouped_shuffling: true

left_pad_source: False
left_pad_target: False

# valid config
fixed_validation_seed: 7

max_tokens_valid: 16384
valid_subset: valid

validate_interval: 500
save_interval: 500
validate_interval_updates: 500
save_interval_updates: 500
keep_interval_updates: 5
keep_last_epochs: 5
keep_best_checkpoints: 5

skip_invalid_size_inputs_valid_test: true

eval_rouge: true
eval_rouge_print_samples: true
eval_rouge_remove_bpe: true
eval_rouge_detok: moses
eval_tokenized_rouge: true
best_checkpoint_metric: rouge-l
maximize_best_checkpoint_metric: true

eval_bleu_args: "'{\"iter_decode_max_iter\": 0, \"iter_decode_with_beam\": 1}'"


# others
user_dir: fs_plugins



